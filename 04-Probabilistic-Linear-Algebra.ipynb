{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2426d0dc-f258-4567-9999-3875389daa05",
   "metadata": {},
   "source": [
    "In lecture 3 we saw how gaussian inference turns into linear-algebra.\n",
    "\n",
    "In lecture 4 we'll see how linear algebra <i>is</i> gaussian inference.\n",
    "\n",
    "why is this useful to know?\n",
    "\n",
    "Given $p(x) = \\mathcal{N}(x; \\mu, \\Sigma)$ then \\\n",
    "$p(x \\mid y = \\mathcal{N}(x; \\mu', \\Sigma')$ \\\n",
    "here $\\mu'$ and $\\Sigma'$ are expressed in terms of `Precision Matrix` or `Gain + Residual + Gram`. This is the most basic machine-learning architecture. It has observations and unkowns which are related by a linear model $y = Ax + b$ \n",
    "\n",
    "what does PyTorch DataLoader do? its a piece of code that goes through the dataset and randomly picks batches of data. why? why not just specific data points? why batches? in linear algebra there are precise answers to these questions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ea9608-8828-4989-b9e1-bfd85f32e0a3",
   "metadata": {},
   "source": [
    "Observations are described by a linear transformation of the distribution variable. \\\n",
    "Measurements can change with time. \\\n",
    "For $n$ observations, there could be $n$ separate affine-transformations $A_{i...n}, \\vec{b}_{i...n}$ \\\n",
    "$A^T\\vec{x} = \\vec{b} \\quad $  where $A \\in \\mathbb{R}^{M \\times N}$, $ \\vec{b} \\in \\mathbb{R}^N , \\vec{x}  \\in \\mathbb{R}^M$ \\\n",
    "Assume $A$ has full-rank (i.e all independant columns) \\\n",
    "Assume $N \\leq M$ i.e we have more unknown parameters than data points. \\\n",
    "Find $\\vec{x} \\in \\mathbb{R}^{M}$ the distribution of parameters that _best_ descirbes the data points "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04d4065-811b-4e3e-91e8-ad2303d583f7",
   "metadata": {},
   "source": [
    "#### Gram-Schmidt\n",
    "The Gram-Schmidt process is a method for turning a set of linearly independent vectors into an orthonormal set that spans the same subspace.\n",
    "\n",
    "Given a span of vectors $\\vec{v_i}$ we need to produce a span of orthonormal vectors $\\vec{u}_i$. \\\n",
    "The idea is to subtract the projection of $\\vec{v}_i\\cdot \\vec{v}_{i+1}$ from $v_{i+1}$ from other \n",
    "\n",
    "1. $\\vec{u}_0 = \\vec{v}_0/\\lVert \\vec{v}_0 \\rVert$\n",
    "2. $\\vec{u}_1 = \\vec{w}_1/\\lVert \\vec{w}_1 \\rVert$  where $\\vec{w}_1 = \\vec{v}_1 - (\\vec{v}_1\\cdot \\vec{u}_0)\\vec{u}_0$\n",
    "3. $\\vec{u}_2 = \\vec{w}_2/\\lVert \\vec{w}_2 \\rVert$  where $\\vec{w}_2 = \\vec{v}_2 - (\\vec{v}_2 \\cdot \\vec{u}_1) \\vec{u}_1 - (\\vec{v}_1 \\cdot \\vec{u}_0 ) \\vec{u}_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff80a48b-16ed-48b0-8e42-c90a119b9c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94c3c951-2577-406a-9238-801941ffadf5",
   "metadata": {},
   "source": [
    "#### Singular Value Decomposition\n",
    "\n",
    "we want to solve $A\\vec{x} = \\vec{b}$ where $A \\in \\mathbb{R}^{M \\times N}, \\vec{b} \\in \\mathbb{R}^M, \\vec{x} \\in \\mathbb{R}^N$ \n",
    "\n",
    "And we want to consider 3 possible case \n",
    "\n",
    "1. $A$ is square, full rank i.e $M = N$. SVD can solve this exactly. \n",
    "2. Overdetermined i.e $M > N$. SVD can find $\\vec{x}$ that minimizes $\\lVert A \\vec{x} - \\vec{b}\\rVert^2$. \n",
    "3. Underdetermined i.e $M<N$ there are infinitely many solutions for $\\vec{x}$ that satisfies $A\\vec{x} = \\vec{b}$ but SVD will give use the __minimum-norm solution__ i.e the $\\vec{x}$ with minimum $\\lVert \\vec{x} \\rVert ^2$\n",
    "\n",
    "#### How SVD works\n",
    "The SVD decomposes $A = Q D U^T$ where \n",
    "- $Q \\in \\mathbb{R}^{M \\times M} $ is orthogonal\n",
    "- $ D \\in \\mathbb{R}^{M \\times N} $ is diagonal\n",
    "- $ U \\in \\mathbb{R}^{N \\times N}$ is orthogonal\n",
    "\n",
    "Now, solving for $\\vec{x} = A^{-1}\\vec{b}$ is just $\\vec{x} = U D^{-1} Q^T \\cdot \\vec{b}$ and since $D$ is diagonal, the $D^{-1}$ is just element-wise reciprocal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b53e820b-0290-4658-bd31-a4653e3aa617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.67900287e+16  5.35800573e+16 -2.67900287e+16]\n",
      "[ 2.97666985e+15 -5.95333970e+15  2.97666985e+15]\n",
      "[-0.96555419  3.93110838 -1.96555419]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The inverse of A is implemented as a standalone method here \n",
    "# because computing the inverse of A is the most time-consuming part of \n",
    "# using SVD to solve linear equations\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "A = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "b1 = np.array([9.1,2,3])\n",
    "b2 = np.array([0.1,2,3])\n",
    "b3 = np.array([1,2,3])\n",
    "\n",
    "def solver(A): # for A in M x N\n",
    "    Q,D,Ut = np.linalg.svd(A.T, full_matrices=False)\n",
    "    return lambda b: Ut.T @ ((Q.T @ b) / D)\n",
    "\n",
    "slv = solver(A)\n",
    "x1 = slv(b1)\n",
    "x2 = slv(b2)\n",
    "x3 = slv(b3)\n",
    "\n",
    "print(x1)\n",
    "print(x2)\n",
    "print(x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c8b9ad-45a1-4c96-baf4-38fe0684b361",
   "metadata": {},
   "source": [
    "now put these ideas aside and imagine you've just had undergrad linear algebra. you know about matrices and inverses but you dont know about SVD. And you're encountering the problerm of evaluating the posterior given a series of observations. \n",
    "\n",
    "$p(z \\mid B^Tz = y + \\epsilon) = \\mathcal{N}(z; m + \\Delta m, V - \\Delta V)$ where \\\n",
    "$\\Delta m = VB(B^TVB + \\Lambda)^{-1}(y - B^Tm)$ \\\n",
    "$\\Delta V = VB(B^TVB + \\Lambda)^{-1}B^TV$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c16de0-48dc-4993-b244-e73d57cef271",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a838a99-618e-49db-b28e-9f5906b795fa",
   "metadata": {},
   "source": [
    "This is actually an imoprtant lecture. I dont fully understand what he's trying to do. but he has an algorithm for computing $p(X \\mid y_1,...y_n)$ i.e the posterior given some observations. But, his algorithm allows you to stop at any $ i\\leq n$. So that's a computational advantage. I might understand it if i came back to it at a later time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb3fa23-7556-46cf-9537-a80464ef2433",
   "metadata": {},
   "source": [
    "#### Here's my attempt at trying to find an expression for $P(x|y_1,...y_n)$\n",
    "\n",
    "$P(x) \\sim \\mathcal{N}(x;\\mu_0,\\Sigma_0)$ \\\n",
    "$y_i = A_i \\cdot x + b_i + \\Lambda $ where $\\Lambda \\sim \\mathcal{N}(0,\\lambda^2)$ is the measurement noise\n",
    "\n",
    "$P(x|y_1) = \\mathcal{N}(x, \\mu_1, \\Sigma_1)$ where $\\mu_1,\\Sigma_1$ are expressed in terms of the `Precision Matrix` or `Gain * Residue` \n",
    "\n",
    "Then $P(x|y_1,y_2) = \\frac{P(y_1 y_2 | x) * P(x)}{P(y_1 y_2)}$ \n",
    "\n",
    "Then $P(x|y_1,y_2) = \\frac{P(y_1 y_2 | x) * P(x)}{\\int P(y_1 y_2 | x)dx}$\n",
    "\n",
    "$P(y_1 y_2 | x) = P(y_1 | x)P(y_2 | x)$ because $y_1,y_2$ are independant measurements when fixed on a conditional $x$ \n",
    "\n",
    "$P(y_1|x)$ and $P(y_2|x)$ are both gaussians with the same variance as $\\Lambda$ with just the mean shifted to $A_i x + b_i$\n",
    "\n",
    "So this can be calculated analytically, complete the squares, you get one big ugly gaussian for $P(x|y_1,y_2)$\n",
    "\n",
    "But the trick is to express $P(x|y_1,y_2)$ in terms of $\\mu_1$ and $\\Sigma_1$ that we computed for $P(x|y_1)$\n",
    "\n",
    "\n",
    "\n",
    "### And here's how he does it \n",
    "_he considers the case where the observation are noiseless, scalar and the transofrmation $A_i$ is just a vector_ i.e $y_i = a_i.x$\n",
    "\n",
    "Computing $P(x|y_1)$ gives us \\\n",
    "$\\Sigma_1 = \\Sigma_0 - \\Sigma_0 A_1 (A_1^T \\Sigma_0 A_1 + \\Lambda)^{-1}(A_1^T \\Sigma_0)$ \n",
    "\n",
    "Now use $P(x|y_1)$ as the __prior__ for the $P(x|y_1 y_2)$. I'm not sure what the correct notation for it would be - perhaps $P((x|y_1) | y_2)$. Then we get \n",
    "\n",
    "$\\Sigma_2 = \\Sigma_1 - \\Sigma_1 A_2 (A_2^T \\Sigma_1 A_2 + \\Lambda)^{-1}(A_2^T \\Sigma_1)$ \n",
    "\n",
    "More generally: \\\n",
    "$\\Sigma_i = \\Sigma_{i-1} - u_{i}\\cdot u_{i}^T$ \\\n",
    "$\\Sigma_i = \\Sigma_0 - \\lbrack u_1\\cdot u_1^T + ... + u_i\\cdot u_i^T \\rbrack$ \\\n",
    "where $u_i = \\frac{1}{\\sqrt{A_i^T \\Sigma_{i-1} A_i}}\\cdot \\Sigma_{i-1} A_i$ \\\n",
    "and $u_i \\Sigma_0^{-1}u_j = \\delta_{i,j}$ \\\n",
    "${u_i}$ is orthonormal under transformation of $\\Sigma_0$ \\\n",
    "$\\Sigma_0$ is a space that spans the original prior distribution of $P(x)$ \\ \n",
    "$A_i$ is just a vector so $u_i \\cdot u_i^T$ is an outer-product not an inner-product. \\\n",
    "_not sure how this generalizes to matrix-transofrmations for observations. \\\n",
    "also he assumes noise-less measurements i.e $\\Lambda = 0$ so I'm not sure if the orthogonality of ${u_i}$ would still be valid in the noisy measurements_\n",
    "\n",
    "This is just rediscovering the __Gram-Schmidt Process__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb290185-9d7e-4460-beb7-9833eba20df0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
